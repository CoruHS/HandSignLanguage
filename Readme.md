This program trains a two stream three dimensional convolutional neural network for sign language recognition starting from random weights. One stream processes RGB frames while the other stream processes classical optical flow computed from the same frames. The model averages the two sets of logits to make a final prediction. No pretrained components or learned optical flow networks are used.

The script imports NumPy, Pandas, PyTorch, OpenCV, and tqdm for progress bars. It attempts to import matplotlib for plotting and sets the plotting handle to None if it is not available so that training still proceeds. It also attempts to import decord for faster video reading and sets a flag that enables a fall back to OpenCV reading when decord is not present.

To keep training stable on shared machines the code limits the number of threads used by common math libraries through environment variables. It also disables OpenCV internal threading and OpenCL where possible. These small steps reduce oversubscription and make performance more predictable.

For reproducibility there is a function that seeds Python, NumPy, and PyTorch and that sets CuDNN into deterministic mode. Another helper quietly chooses the best available device in the order CUDA, Apple MPS, then CPU. None of this produces noisy printing.

Videos can have different lengths, so the script needs a consistent way to sample frames. The frame sampler selects a temporal window and then takes evenly spaced indices within that window. During training the window is chosen randomly to introduce variety. During validation and testing the window is centered to remove randomness. This preserves motion while keeping the compute budget fixed.

Reading videos and forming tensors works in a single place. If decord is enabled the reader uses it to fetch a batch of frames directly as tensors. Otherwise it uses OpenCV to read frames as arrays. The clip is resized to a square and cropped. Training uses a random crop while evaluation uses a center crop. The result is converted into a tensor shaped as channels by time by height by width and normalized to the range minus one to one.

Optical flow is computed with the classical Farneback method from OpenCV. The function converts the normalized clip back to bytes, computes dense flow between consecutive frames, and applies a tanh to keep extreme motion values from dominating. The result is a two channel tensor representing horizontal and vertical motion. If the flow has one fewer time step than the RGB clip the last flow frame is repeated so that shapes align.

The dataset classes handle both training and testing. The training dataset reads filenames and labels from a CSV and returns an RGB clip, a flow clip, and the corresponding label. The test dataset walks a directory for common video extensions and returns an RGB clip, a flow clip, and the filename. DataLoaders are configured with shuffling for training, a fixed order for validation and testing, and pinned memory plus non blocking device transfers for speed.

The model uses a small and readable architecture. A basic block contains a three dimensional convolution followed by batch normalization and ReLU. Each stream uses a shallow stack of these blocks with occasional pooling, then an adaptive pooling layer to reduce to a single cell, and finally a linear classifier. Weights for convolutions are initialized with Kaiming initialization so that training starts in a sane regime. The fusion model simply builds one stream for RGB and one stream for flow and averages their logits.

There are small helper functions for metrics and splitting. A confusion matrix is constructed with NumPy indexed adds. Macro F1 is computed from that matrix. Accuracy is derived from logits by taking argmax. A stratified split function creates training and validation file lists that preserve label proportions. Class weights are computed from the training subset only so that validation statistics are not used for training choices.

The training step is straightforward. For each batch the model performs a forward pass on both streams, then computes the main loss and two auxiliary losses on the individual streams and combines them. If CUDA is available the step uses automatic mixed precision and a gradient scaler to improve speed and memory use. Gradients are clipped to a modest norm to avoid spikes. A running loss and accuracy are shown with a tqdm bar. Evaluation uses the same loss recipe but does not backpropagate and collects predictions to compute macro F1.

The main function wires everything together. It parses command line arguments for paths, sizes, and optimization hyperparameters. It loads the label CSV, determines the number of classes, builds the stratified split, and constructs the datasets and loaders. It computes class weights from the training portion, creates the model, and sets up the optimizer with AdamW, the cosine annealing scheduler, and cross entropy with label smoothing. Each epoch runs a training pass and then a validation pass, steps the scheduler, tracks the best validation accuracy, and saves the best checkpoint when it improves. A simple history CSV is written so you can plot or audit results later.

After training the script reloads the best checkpoint and computes a confusion matrix for the validation set. It saves the raw matrix as a CSV. If matplotlib is installed it also saves heatmap images for the raw matrix and a row normalized version. A small text file records macro F1 and accuracy for quick reference.

Finally the code builds a test dataset and runs inference with the trained model to produce a submission CSV that maps each filename to a predicted label. This inference path uses the same preprocessing and frame sampling as training so results are consistent across runs.
